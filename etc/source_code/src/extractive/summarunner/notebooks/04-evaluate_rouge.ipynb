{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate using ROUGE Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rouge\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "# from rouge_score import rouge_scorer, scoring\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RougeScorer:\n",
    "    def __init__(self, use_tokenizer=True):\n",
    "\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        if use_tokenizer:\n",
    "            self.tokenizer = Mecab()\n",
    "\n",
    "        self.rouge_evaluator = rouge.Rouge(\n",
    "            metrics=[\"rouge-n\", \"rouge-l\"],\n",
    "            max_n=2,\n",
    "            limit_length=True,\n",
    "            length_limit=1000,\n",
    "            length_limit_type=\"words\",\n",
    "            apply_avg=True,\n",
    "            apply_best=False,\n",
    "            alpha=0.5,  # Default F1_score\n",
    "            weight_factor=1.2,\n",
    "            stemming=True,\n",
    "        )\n",
    "\n",
    "    def compute_rouge(self, ref_path, hyp_path):\n",
    "        ref_fnames = glob(f\"{ref_path}/*.txt\")\n",
    "        hyp_fnames = glob(f\"{hyp_path}/*.txt\")\n",
    "        ref_fnames.sort()\n",
    "        hyp_fnames.sort()\n",
    "\n",
    "        self.reference_summaries = []\n",
    "        self.generated_summaries = []\n",
    "\n",
    "        for ref_fname, hyp_fname in tqdm(\n",
    "            zip(ref_fnames, hyp_fnames), total=len(ref_fnames)\n",
    "        ):\n",
    "            assert os.path.split(ref_fname)[1] == os.path.split(hyp_fname)[1]\n",
    "\n",
    "            with open(ref_fname, \"r\", encoding=\"utf8\") as f:\n",
    "                ref = f.read().split(\"\\n\")\n",
    "                ref = \"\".join(ref)\n",
    "\n",
    "            with open(hyp_fname, \"r\", encoding=\"utf8\") as f:\n",
    "                hyp = f.read().split(\"\\n\")\n",
    "                hyp = \"\".join(hyp)\n",
    "\n",
    "            if self.use_tokenizer:\n",
    "                ref = self.tokenizer.morphs(ref)\n",
    "                hyp = self.tokenizer.morphs(hyp)\n",
    "\n",
    "            ref = \" \".join(ref)\n",
    "            hyp = \" \".join(hyp)\n",
    "\n",
    "            self.reference_summaries.append(ref)\n",
    "            self.generated_summaries.append(hyp)\n",
    "\n",
    "        scores = self.rouge_evaluator.get_scores(\n",
    "            self.generated_summaries, self.reference_summaries\n",
    "        )\n",
    "        str_scores = self.format_rouge_scores(scores)\n",
    "        self.save_rouge_scores(str_scores)\n",
    "        return str_scores\n",
    "\n",
    "    def save_rouge_scores(self, str_scores):\n",
    "        with open(\"rouge_scores.txt\", \"w\") as output:\n",
    "            output.write(str_scores)\n",
    "\n",
    "    def format_rouge_scores(self, scores):\n",
    "        return \"\"\"\\n\n",
    "    ****** ROUGE SCORES ******\n",
    "    ** ROUGE 1\n",
    "    F1        >> {:.3f}\n",
    "    Precision >> {:.3f}\n",
    "    Recall    >> {:.3f}\n",
    "    ** ROUGE 2\n",
    "    F1        >> {:.3f}\n",
    "    Precision >> {:.3f}\n",
    "    Recall    >> {:.3f}\n",
    "    ** ROUGE L\n",
    "    F1        >> {:.3f}\n",
    "    Precision >> {:.3f}\n",
    "    Recall    >> {:.3f}\"\"\".format(\n",
    "            scores[\"rouge-1\"][\"f\"],\n",
    "            scores[\"rouge-1\"][\"p\"],\n",
    "            scores[\"rouge-1\"][\"r\"],\n",
    "            scores[\"rouge-2\"][\"f\"],\n",
    "            scores[\"rouge-2\"][\"p\"],\n",
    "            scores[\"rouge-2\"][\"r\"],\n",
    "            scores[\"rouge-l\"][\"f\"],\n",
    "            scores[\"rouge-l\"][\"p\"],\n",
    "            scores[\"rouge-l\"][\"r\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Rouge:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "#         use_tokenizer=False,\n",
    "#         use_agregator=True,\n",
    "#     ):\n",
    "\n",
    "#         self.use_tokenizer = use_tokenizer\n",
    "#         if use_tokenizer:\n",
    "#             self.tokenizer = Mecab()\n",
    "\n",
    "#         self.use_agregator = use_agregator\n",
    "#         self.scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types)\n",
    "#         if use_agregator:\n",
    "#             self.agregator = scoring.BootstrapAggregator()\n",
    "#         else:\n",
    "#             self.scores = []\n",
    "\n",
    "#     def rouge_score(self, ref_path, hyp_path):\n",
    "#         ref_fnames = glob(f\"{ref_path}/*.txt\")\n",
    "#         hyp_fnames = glob(f\"{hyp_path}/*.txt\")\n",
    "#         ref_fnames.sort()\n",
    "#         hyp_fnames.sort()\n",
    "\n",
    "#         self.ref_list = []\n",
    "#         self.hyp_list = []\n",
    "\n",
    "#         for ref_fname, hyp_fname in tqdm(\n",
    "#             zip(ref_fnames, hyp_fnames), total=len(ref_fnames)\n",
    "#         ):\n",
    "#             assert os.path.split(ref_fname)[1] == os.path.split(hyp_fname)[1]\n",
    "\n",
    "#             with open(ref_fname, \"r\", encoding=\"utf8\") as f:\n",
    "#                 ref = f.read().split(\"\\n\")\n",
    "#                 ref = \"\".join(ref)\n",
    "\n",
    "#             with open(hyp_fname, \"r\", encoding=\"utf8\") as f:\n",
    "#                 hyp = f.read().split(\"\\n\")\n",
    "#                 hyp = \"\".join(hyp)\n",
    "\n",
    "#             if self.use_tokenizer:\n",
    "#                 ref = self.tokenizer.morphs(ref)\n",
    "#                 hyp = self.tokenizer.morphs(hyp)\n",
    "\n",
    "#             ref = \" \".join(ref)\n",
    "#             hyp = \" \".join(hyp)\n",
    "\n",
    "#             self.ref_list.append(ref)\n",
    "#             self.hyp_list.append(hyp)\n",
    "\n",
    "#             score = self.scorer.score(ref, hyp)\n",
    "#             if self.use_agregator:\n",
    "#                 self.agregator.add_scores(score)\n",
    "#             else:\n",
    "#                 self.scores.append(score)\n",
    "\n",
    "#         if self.use_agregator:\n",
    "#             result = self.agregator.aggregate()\n",
    "#         else:\n",
    "#             result = {}\n",
    "#             for key in self.scores[0]:\n",
    "#                 result[key] = list(score[key] for score in self.scores)\n",
    "\n",
    "#         return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_eval = RougeScorer(use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1486.70it/s]\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"../outputs/ext_ref\"\n",
    "hyp_path = \"../outputs/hyp\"\n",
    "\n",
    "result = rouge_eval.compute_rouge(ref_path, hyp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyp vs ext_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    ****** ROUGE SCORES ******\n",
      "    ** ROUGE 1\n",
      "    F1        >> 0.651\n",
      "    Precision >> 0.653\n",
      "    Recall    >> 0.718\n",
      "    ** ROUGE 2\n",
      "    F1        >> 0.502\n",
      "    Precision >> 0.507\n",
      "    Recall    >> 0.551\n",
      "    ** ROUGE L\n",
      "    F1        >> 0.673\n",
      "    Precision >> 0.671\n",
      "    Recall    >> 0.729\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyp vs abs_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    ****** ROUGE SCORES ******\n",
      "    ** ROUGE 1\n",
      "    F1        >> 0.421\n",
      "    Precision >> 0.349\n",
      "    Recall    >> 0.668\n",
      "    ** ROUGE 2\n",
      "    F1        >> 0.188\n",
      "    Precision >> 0.156\n",
      "    Recall    >> 0.302\n",
      "    ** ROUGE L\n",
      "    F1        >> 0.433\n",
      "    Precision >> 0.363\n",
      "    Recall    >> 0.638\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ext_ref vs abs_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    ****** ROUGE SCORES ******\n",
      "    ** ROUGE 1\n",
      "    F1        >> 0.556\n",
      "    Precision >> 0.470\n",
      "    Recall    >> 0.793\n",
      "    ** ROUGE 2\n",
      "    F1        >> 0.276\n",
      "    Precision >> 0.234\n",
      "    Recall    >> 0.395\n",
      "    ** ROUGE L\n",
      "    F1        >> 0.552\n",
      "    Precision >> 0.472\n",
      "    Recall    >> 0.746\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
