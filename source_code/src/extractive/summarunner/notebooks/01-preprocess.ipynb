{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import platform\n",
    "import pickle\n",
    "import dill\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TestTubeLogger  # pip install test-tube\n",
    "\n",
    "from functools import partial\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import SummaRunner\n",
    "\n",
    "# from utils.data import SumDataset, Feature\n",
    "from model import build_vocab\n",
    "from model.types_ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        from eunjeon import Mecab\n",
    "    except:\n",
    "        print(\"please install eunjeon module\")\n",
    "else:  # Ubuntu일 경우\n",
    "    from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../../../../datasets/kor_data/magazine/train.jsonl\"\n",
    "dev_path = \"../../../../datasets/kor_data/total_data/dev.jsonl\"\n",
    "test_path = \"../../../../datasets/kor_data/total_data/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    jsonl = list(f)\n",
    "\n",
    "train_data = []\n",
    "for json_str in jsonl:\n",
    "    train_data.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'media': '이코노미스트',\n",
       " 'id': '330370',\n",
       " 'article_original': ['경영위기에 놓인 쌍용자동차를 놓고 최대주주인 인도 마힌드라 그룹과 산업은행 간 줄다리기가 이어지고 있다.',\n",
       "  '마힌드라가 쌍용차 지배권 포기를 언급한 가운데, 산업은행은 쌍용차에 대한 기간산업안정기금 지원에 선을 그었다.',\n",
       "  '지난 12일(현지시간) 쌍용차 이사회 의장인 파완고엔카 마힌드라 사장은 마힌드라의 컨퍼런스콜에서 \"쌍용차는 새로운 투자자를 필요로 한다\"며 \"투자자를 확보할 수 있을지 모색 중\"이라고 말했다.',\n",
       "  '이 발언은 쌍용차에 대한 마힌드라의 투자 결정 철회와 맞물려 사업 철수에 대한 불안감을 키우고 있다.',\n",
       "  '마힌드라는 애초 3년 후 흑자전환 목표를 내걸고 쌍용차에 2300억원 투자 계획을 제시했다가 철회했다.',\n",
       "  '대신 긴급 자금 400억원만 지원하기로 했다.',\n",
       "  '이후 산업계에선 정부가 쌍용차에 기간산업안정기금을 지원할 수 있다는 의견이 나왔다.',\n",
       "  '이에 대해 최대현 산은 기업금융부문 부행장은 6월 17일 \"기간산업안정기금은 코로나19 사태 이전부터 경영에 문제가 있는 회사를 지원하는 것은 아니다\"라며 선을 그었다.',\n",
       "  '다만 다음 달 만기가 도래하는 900억원의 대출은 만기연장을 할 예정이다.'],\n",
       " 'abstractive': '지난 12일 쌍용차 파완고엔카 사장은 컨퍼런스 콜에서 \"쌍용차는 새 투자자를 기다린다\"고 말했는데 그는 3년의 흑자전환의 목표를 세우고 회사에 2300억원을 투자하려다 이를 철회하게 되므로 산업계에서는 정부가 쌍용차에 안정기금을 지원하기를 기대한다.',\n",
       " 'extractive': [2, 4, 6]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(\n",
    "    dataset: JSONType, stopwords: Optional[List[str]] = None, num_words: int = 40000\n",
    "):\n",
    "    # 0. tokenizer\n",
    "    tokenizer = Mecab()\n",
    "\n",
    "    # 1. tokenization\n",
    "    all_tokens = []\n",
    "    for data in tqdm(dataset):\n",
    "        sents = data[\"article_original\"]\n",
    "        for sent in sents:\n",
    "            tokens = tokenizer.morphs(sent)\n",
    "            if stopwords:\n",
    "                all_tokens.extend([token for token in tokens if token not in stopwords])\n",
    "            else:\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "    # 2. build vocab\n",
    "    vocab = Counter(all_tokens)\n",
    "    vocab = vocab.most_common(num_words)\n",
    "\n",
    "    # 3. add pad & unk tokens\n",
    "    word_index = defaultdict()\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<UNK>\"] = 1\n",
    "\n",
    "    for idx, (word, _) in enumerate(vocab, 2):\n",
    "        word_index[word] = idx\n",
    "\n",
    "    index_word = {idx: word for word, idx in word_index.items()}\n",
    "\n",
    "    return word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53265/53265 [01:27<00:00, 608.73it/s]\n"
     ]
    }
   ],
   "source": [
    "word_index, index_word = build_vocab(train_data)\n",
    "\n",
    "with open(\"./word_index_magazine.pkl\", \"wb\") as f:\n",
    "    dill.dump(word_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../utils//word_index_v02.pkl\", \"rb\") as f:\n",
    "#     word_index = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            jsonl = list(f)\n",
    "\n",
    "        self.data = []\n",
    "        for json_str in jsonl:\n",
    "            self.data.append(json.loads(json_str))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.data[idx][\"article_original\"]\n",
    "        ext_indices = self.data[idx][\"extractive\"]\n",
    "        summaries = self.data[idx][\"abstractive\"]\n",
    "\n",
    "        return doc, ext_indices, summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SumDataset(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class Feature:\n",
    "#     def __init__(self, word_index, tokenizer):\n",
    "#         self.word_index = word_index\n",
    "#         self.index_word = {idx: word for word, idx in word_index.items()}\n",
    "#         assert len(self.word_index) == len(self.index_word)\n",
    "#         self.PAD_IDX = 0\n",
    "#         self.UNK_IDX = 1\n",
    "#         self.PAD_TOKEN = \"<PAD>\"\n",
    "#         self.UNK_TOKEN = \"<UNK>\"\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.word_index)\n",
    "\n",
    "#     def index_to_word(self, idx):\n",
    "#         return self.index_word[idx]\n",
    "\n",
    "#     def word_to_index(self, w):\n",
    "#         if w in self.word_index:\n",
    "#             return self.word_index[w]\n",
    "#         else:\n",
    "#             return self.UNK_IDX\n",
    "\n",
    "#     ###################\n",
    "#     # Create Features #\n",
    "#     ###################\n",
    "#     def make_features(\n",
    "#         self,\n",
    "#         docs,\n",
    "#         ext_idx_list,\n",
    "#         summaries_list,\n",
    "#         doc_trunc=100,\n",
    "#         sent_trunc=128,\n",
    "#         split_token=\"\\n\",\n",
    "#     ):\n",
    "\n",
    "#         # trunc document\n",
    "#         # 문서 내 doc_trunc 문장 개수까지 가져옴\n",
    "#         sents_list, targets, doc_lens, ext_sums, abs_sums = [], [], [], [], []\n",
    "#         for doc, ext_indices, abs_sum in zip(docs, ext_idx_list, summaries_list):\n",
    "#             labels = []\n",
    "#             for idx in range(len(doc)):\n",
    "#                 if idx in ext_indices:\n",
    "#                     labels.append(1)\n",
    "#                 else:\n",
    "#                     labels.append(0)\n",
    "\n",
    "#             max_sent_num = min(doc_trunc, len(doc))\n",
    "#             sents = doc[:max_sent_num]\n",
    "#             labels = labels[:max_sent_num]\n",
    "#             ext_sum = [sent for sent, label in zip(sents, labels) if label == 1]\n",
    "\n",
    "#             sents_list.append(sents)\n",
    "#             targets.append(labels)\n",
    "#             doc_lens.append(len(sents))\n",
    "#             ext_sums.append(ext_sum)\n",
    "#             abs_sums.append(abs_sum)\n",
    "\n",
    "#         # trunc or pad sent\n",
    "#         # 문장 내 sent_trunc 단어 개수까지 가져옴\n",
    "#         max_sent_len = 0\n",
    "#         batch_sents = []\n",
    "#         features_list = []\n",
    "#         for sents in sents_list:\n",
    "#             for sent in sents:\n",
    "#                 words = self.tokenizer.morphs(sent)\n",
    "#                 # words = [word for word in words if len(word) > 1]\n",
    "#                 if len(words) > sent_trunc:\n",
    "#                     words = words[:sent_trunc]\n",
    "#                 max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
    "#                 batch_sents.append(words)\n",
    "\n",
    "#             features = []\n",
    "#             for sent in batch_sents:\n",
    "#                 feature = [self.PAD_IDX for _ in range(max_sent_len - len(sent))] + [\n",
    "#                     self.word_to_index(w) for w in sent\n",
    "#                 ]\n",
    "#                 features.append(feature)\n",
    "\n",
    "#             features_list.append(features)\n",
    "\n",
    "#         return features_list, targets, doc_lens, ext_sums, abs_sums, docs\n",
    "\n",
    "#     def make_predict_features(\n",
    "#         self, docs, sent_trunc=128, doc_trunc=100, split_token=\". \",\n",
    "#     ):\n",
    "\n",
    "#         sents_list, doc_lens = [], []\n",
    "#         for doc in docs:\n",
    "#             sents = doc.split(split_token)\n",
    "#             max_sent_num = min(doc_trunc, len(sents))\n",
    "#             sents = sents[:max_sent_num]\n",
    "#             sents_list.extend(sents)\n",
    "#             doc_lens.append(len(sents))\n",
    "\n",
    "#         # trunc or pad sent\n",
    "#         max_sent_len = 0\n",
    "#         batch_sents = []\n",
    "#         for sent in sents_list:\n",
    "#             words = self.tokenizer.morphs(sent)\n",
    "#             # words = [word for word in words if len(word) > 1]\n",
    "#             if len(words) > sent_trunc:\n",
    "#                 words = words[:sent_trunc]\n",
    "#             max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
    "#             batch_sents.append(words)\n",
    "\n",
    "#         features = []\n",
    "#         for sent in batch_sents:\n",
    "#             feature = [self.PAD_IDX for _ in range(max_sent_len - len(sent))] + [\n",
    "#                 self.word_to_index(w) for w in sent\n",
    "#             ]\n",
    "#             features.append(feature)\n",
    "\n",
    "#         return features, doc_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    def __init__(self, word_index, tokenizer):\n",
    "        self.word_index = word_index\n",
    "        self.index_word = {idx: word for word, idx in word_index.items()}\n",
    "        assert len(self.word_index) == len(self.index_word)\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.PAD_TOKEN = \"<PAD>\"\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_index)\n",
    "\n",
    "    def index_to_word(self, idx):\n",
    "        return self.index_word[idx]\n",
    "\n",
    "    def word_to_index(self, w):\n",
    "        if w in self.word_index:\n",
    "            return self.word_index[w]\n",
    "        else:\n",
    "            return self.UNK_IDX\n",
    "\n",
    "    ###################\n",
    "    # Create Features #\n",
    "    ###################\n",
    "    def make_features(\n",
    "        self,\n",
    "        docs,\n",
    "        ext_idx_list,\n",
    "        summaries_list,\n",
    "        doc_trunc=50,\n",
    "        sent_trunc=128,\n",
    "        split_token=\"\\n\",\n",
    "    ):\n",
    "\n",
    "        # trunc document\n",
    "        # 문서 내 doc_trunc 문장 개수까지 가져옴\n",
    "        sents_list, targets, doc_lens, ext_sums, abs_sums = [], [], [], [], []\n",
    "        for doc, ext_indices, abs_sum in zip(docs, ext_idx_list, summaries_list):\n",
    "            labels = []\n",
    "            for idx in range(len(doc)):\n",
    "                if idx in ext_indices:\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "\n",
    "            max_sent_num = min(doc_trunc, len(doc))\n",
    "            sents = doc[:max_sent_num]\n",
    "            labels = labels[:max_sent_num]\n",
    "            ext_sum = [sent for sent, label in zip(sents, labels) if label == 1]\n",
    "\n",
    "            sents_list.extend(sents)\n",
    "            targets.extend(labels)\n",
    "            doc_lens.append(len(sents))\n",
    "            ext_sums.append(ext_sum)\n",
    "            abs_sums.append(abs_sum)\n",
    "\n",
    "        # trunc or pad sent\n",
    "        # 문장 내 sent_trunc 단어 개수까지 가져옴\n",
    "        max_sent_len = 0\n",
    "        batch_sents = []\n",
    "        for sent in sents_list:\n",
    "            words = self.tokenizer.morphs(sent)\n",
    "            # words = [word for word in words if len(word) > 1]\n",
    "            if len(words) > sent_trunc:\n",
    "                words = words[:sent_trunc]\n",
    "            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
    "            batch_sents.append(words)\n",
    "\n",
    "        features = []\n",
    "        for sent in batch_sents:\n",
    "            feature = [self.PAD_IDX for _ in range(max_sent_len - len(sent))] + [\n",
    "                self.word_to_index(w) for w in sent\n",
    "            ]\n",
    "            features.append(feature)\n",
    "\n",
    "        return features, targets, doc_lens, ext_sums, abs_sums, docs\n",
    "\n",
    "    def make_predict_features(\n",
    "        self, docs, sent_trunc=128, doc_trunc=50, split_token=\". \",\n",
    "    ):\n",
    "\n",
    "        sents_list, doc_lens = [], []\n",
    "        for doc in docs:\n",
    "            sents = doc.split(split_token)\n",
    "            max_sent_num = min(doc_trunc, len(sents))\n",
    "            sents = sents[:max_sent_num]\n",
    "            sents_list.extend(sents)\n",
    "            doc_lens.append(len(sents))\n",
    "\n",
    "        # trunc or pad sent\n",
    "        max_sent_len = 0\n",
    "        batch_sents = []\n",
    "        for sent in sents_list:\n",
    "            words = self.tokenizer.morphs(sent)\n",
    "            # words = [word for word in words if len(word) > 1]\n",
    "            if len(words) > sent_trunc:\n",
    "                words = words[:sent_trunc]\n",
    "            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
    "            batch_sents.append(words)\n",
    "\n",
    "        features = []\n",
    "        for sent in batch_sents:\n",
    "            feature = [self.PAD_IDX for _ in range(max_sent_len - len(sent))] + [\n",
    "                self.word_to_index(w) for w in sent\n",
    "            ]\n",
    "            features.append(feature)\n",
    "\n",
    "        return features, doc_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def collate_fn(batch, feature):\n",
    "#     docs = [entry[0] for entry in batch]\n",
    "#     labels_list = [entry[1] for entry in batch]\n",
    "#     summaries_list = [entry[2] for entry in batch]\n",
    "\n",
    "#     features, targets, doc_lens, ext_sums, abs_sums, docs = feature.make_features(\n",
    "#         docs, labels_list, summaries_list\n",
    "#     )\n",
    "\n",
    "#     #     features = torch.LongTensor(features)\n",
    "#     #     targets = torch.FloatTensor(targets)\n",
    "#     max_doc_len = max(doc_lens)\n",
    "#     doc_lens = torch.LongTensor(doc_lens)\n",
    "#     return features, targets, doc_lens, max_doc_len, ext_sums, abs_sums, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, feature):\n",
    "    docs = [entry[0] for entry in batch]\n",
    "    labels_list = [entry[1] for entry in batch]\n",
    "    summaries_list = [entry[2] for entry in batch]\n",
    "\n",
    "    features, targets, doc_lens, ext_sums, abs_sums, docs = feature.make_features(\n",
    "        docs, labels_list, summaries_list\n",
    "    )\n",
    "\n",
    "    docs = []\n",
    "    labels = []\n",
    "    start = 0\n",
    "    pad_dim = len(features[0])\n",
    "    max_doc_len = max(doc_lens)\n",
    "    for doc_len in doc_lens:\n",
    "        stop = start + doc_len\n",
    "        doc = features[start:stop]\n",
    "        target = targets[start:stop]\n",
    "        start = stop\n",
    "\n",
    "        doc = torch.LongTensor(doc)\n",
    "        if len(doc) == max_doc_len:\n",
    "            docs.append(doc.unsqueeze(0))\n",
    "        else:\n",
    "            pad = torch.zeros(max_doc_len - doc_len, pad_dim, dtype=torch.long)\n",
    "            docs.append(torch.cat([doc, pad]).unsqueeze(0))\n",
    "\n",
    "        if len(target) == max_doc_len:\n",
    "            labels.append(torch.FloatTensor(target).unsqueeze(0))\n",
    "        else:\n",
    "            pad = torch.zeros(max_doc_len - doc_len)\n",
    "            target = torch.FloatTensor(target)\n",
    "            labels.append(torch.cat([target, pad]).unsqueeze(0))\n",
    "\n",
    "    docs = torch.cat(docs, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    targets = torch.FloatTensor(targets)\n",
    "    doc_lens = torch.LongTensor(doc_lens)\n",
    "    return docs, labels, doc_lens, max_doc_len, ext_sums, abs_sums, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature class\n",
    "mecab = Mecab()\n",
    "feature = Feature(word_index, mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_fn, feature=feature),\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    docs, targets, doc_lens, max_doc_len, ext_sums, abs_sums, docs = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for idx, doc_len in enumerate(doc_lens):\n",
    "    doc = targets[idx][:doc_len]\n",
    "    labels.append(doc)\n",
    "labels = torch.cat(labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([447])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2594, 0.0480, 0.4768,  ..., 0.6116, 0.2827, 0.9941],\n",
       "        [0.6936, 0.9198, 0.2577,  ..., 0.8877, 0.9924, 0.9541],\n",
       "        [0.1422, 0.2006, 0.9687,  ..., 0.3762, 0.4070, 0.9753],\n",
       "        ...,\n",
       "        [0.9653, 0.6189, 0.8120,  ..., 0.7876, 0.7225, 0.4006],\n",
       "        [0.1173, 0.7142, 0.3717,  ..., 0.5497, 0.9576, 0.7127],\n",
       "        [0.2075, 0.5802, 0.3070,  ..., 0.6827, 0.2326, 0.0602]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42937"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
