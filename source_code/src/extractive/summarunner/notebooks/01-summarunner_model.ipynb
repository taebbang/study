{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import platform\n",
    "import pickle\n",
    "import dill\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TestTubeLogger  # pip install test-tube\n",
    "\n",
    "from functools import partial\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import SummaRunner\n",
    "\n",
    "# from utils.data import SumDataset, Feature\n",
    "from model import build_vocab\n",
    "from model.types_ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        from eunjeon import Mecab\n",
    "    except:\n",
    "        print(\"please install eunjeon module\")\n",
    "else:  # Ubuntu일 경우\n",
    "    from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../../../../datasets/kor_data/total_data/train_50965.jsonl\"\n",
    "dev_path = \"../../../../datasets/kor_data/total_data/dev_50965.jsonl\"\n",
    "test_path = \"../../../../datasets/kor_data/total_data/test_50965.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    jsonl = list(f)\n",
    "\n",
    "train_data = []\n",
    "for json_str in jsonl:\n",
    "    train_data.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(\n",
    "    dataset: JSONType, stopwords: Optional[List[str]] = None, num_words: int = 40000\n",
    "):\n",
    "    # 0. tokenizer\n",
    "    tokenizer = Mecab()\n",
    "\n",
    "    # 1. tokenization\n",
    "    all_tokens = []\n",
    "    for data in tqdm(dataset):\n",
    "        sents = data[\"article_original\"]\n",
    "        for sent in sents:\n",
    "            tokens = tokenizer.morphs(sent)\n",
    "            if stopwords:\n",
    "                all_tokens.extend([token for token in tokens if token not in stopwords])\n",
    "            else:\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "    # 2. build vocab\n",
    "    vocab = Counter(all_tokens)\n",
    "    vocab = vocab.most_common(num_words)\n",
    "\n",
    "    # 3. add pad & unk tokens\n",
    "    word_index = defaultdict()\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<UNK>\"] = 1\n",
    "\n",
    "    for idx, (word, _) in enumerate(vocab, 2):\n",
    "        word_index[word] = idx\n",
    "\n",
    "    index_word = {idx: word for word, idx in word_index.items()}\n",
    "\n",
    "    return word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index, index_word = build_vocab(train_data)\n",
    "\n",
    "# with open(\"./word_index_v02.pkl\", \"wb\") as f:\n",
    "#     dill.dump(word_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../utils//word_index_v02.pkl\", \"rb\") as f:\n",
    "    word_index = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            jsonl = list(f)\n",
    "\n",
    "        self.data = []\n",
    "        for json_str in jsonl:\n",
    "            self.data.append(json.loads(json_str))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.data[idx][\"article_original\"]\n",
    "        ext_indices = self.data[idx][\"extractive\"]\n",
    "        summaries = self.data[idx][\"abstractive\"]\n",
    "\n",
    "        return doc, ext_indices, summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SumDataset(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    def __init__(self, word_index, tokenizer):\n",
    "        self.word_index = word_index\n",
    "        self.index_word = {idx: word for word, idx in word_index.items()}\n",
    "        assert len(self.word_index) == len(self.index_word)\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.PAD_TOKEN = \"<PAD>\"\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_index)\n",
    "\n",
    "    def index_to_word(self, idx):\n",
    "        return self.index_word[idx]\n",
    "\n",
    "    def word_to_index(self, w):\n",
    "        if w in self.word_index:\n",
    "            return self.word_index[w]\n",
    "        else:\n",
    "            return self.UNK_IDX\n",
    "\n",
    "    ###################\n",
    "    # Create Features #\n",
    "    ###################\n",
    "    def make_features(\n",
    "        self,\n",
    "        docs,\n",
    "        ext_idx_list,\n",
    "        summaries_list,\n",
    "        doc_trunc=100,\n",
    "        sent_trunc=128,\n",
    "        split_token=\"\\n\",\n",
    "    ):\n",
    "\n",
    "        # trunc document\n",
    "        # 문서 내 doc_trunc 문장 개수까지 가져옴\n",
    "        sents_list, targets, doc_lens, ext_sums, abs_sums = [], [], [], [], []\n",
    "        for doc, ext_indices, abs_sum in zip(docs, ext_idx_list, summaries_list):\n",
    "            labels = []\n",
    "            for idx in range(len(doc)):\n",
    "                if idx in ext_indices:\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "\n",
    "            max_sent_num = min(doc_trunc, len(doc))\n",
    "            sents = doc[:max_sent_num]\n",
    "            labels = labels[:max_sent_num]\n",
    "            ext_sum = [sent for sent, label in zip(sents, labels) if label == 1]\n",
    "\n",
    "            sents_list.append(sents)\n",
    "            targets.append(labels)\n",
    "            doc_lens.append(len(sents))\n",
    "            ext_sums.append(ext_sum)\n",
    "            abs_sums.append(abs_sum)\n",
    "\n",
    "        # trunc or pad sent\n",
    "        # 문장 내 sent_trunc 단어 개수까지 가져옴\n",
    "        max_sent_len = 0\n",
    "        batch_sents = []\n",
    "        features_list = []\n",
    "        for sents in sents_list:\n",
    "            for sent in sents:\n",
    "                words = self.tokenizer.morphs(sent)\n",
    "                # words = [word for word in words if len(word) > 1]\n",
    "                if len(words) > sent_trunc:\n",
    "                    words = words[:sent_trunc]\n",
    "                max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
    "                batch_sents.append(words)\n",
    "\n",
    "            features = []\n",
    "            for sent in batch_sents:\n",
    "                feature = [self.PAD_IDX for _ in range(max_sent_len - len(sent))] + [\n",
    "                    self.word_to_index(w) for w in sent\n",
    "                ]\n",
    "                features.append(feature)\n",
    "\n",
    "            features_list.append(features)\n",
    "\n",
    "        return features, targets, doc_lens, ext_sums, abs_sums, docs\n",
    "\n",
    "    def make_predict_features(\n",
    "        self, docs, sent_trunc=128, doc_trunc=100, split_token=\". \",\n",
    "    ):\n",
    "\n",
    "        sents_list, doc_lens = [], []\n",
    "        for doc in docs:\n",
    "            sents = doc.split(split_token)\n",
    "            max_sent_num = min(doc_trunc, len(sents))\n",
    "            sents = sents[:max_sent_num]\n",
    "            sents_list.extend(sents)\n",
    "            doc_lens.append(len(sents))\n",
    "\n",
    "        # trunc or pad sent\n",
    "        max_sent_len = 0\n",
    "        batch_sents = []\n",
    "        for sent in sents_list:\n",
    "            words = self.tokenizer.morphs(sent)\n",
    "            # words = [word for word in words if len(word) > 1]\n",
    "            if len(words) > sent_trunc:\n",
    "                words = words[:sent_trunc]\n",
    "            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
    "            batch_sents.append(words)\n",
    "\n",
    "        features = []\n",
    "        for sent in batch_sents:\n",
    "            feature = [self.PAD_IDX for _ in range(max_sent_len - len(sent))] + [\n",
    "                self.word_to_index(w) for w in sent\n",
    "            ]\n",
    "            features.append(feature)\n",
    "\n",
    "        return features, doc_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, feature):\n",
    "    docs = [entry[0] for entry in batch]\n",
    "    labels_list = [entry[1] for entry in batch]\n",
    "    summaries_list = [entry[2] for entry in batch]\n",
    "\n",
    "    features, targets, doc_lens, ext_sums, abs_sums, docs = feature.make_features(\n",
    "        docs, labels_list, summaries_list\n",
    "    )\n",
    "\n",
    "    features = torch.LongTensor(features)\n",
    "    #     targets = torch.FloatTensor(targets)\n",
    "    max_doc_len = max(doc_lens)\n",
    "    doc_lens = torch.LongTensor(doc_lens)\n",
    "    return features, targets, doc_lens, max_doc_len, ext_sums, abs_sums, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature class\n",
    "mecab = Mecab()\n",
    "feature = Feature(word_index, mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_fn, feature=feature),\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    features, targets, doc_lens, max_doc_len, ext_sums, abs_sums, docs = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 73])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
